{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BucketIterator' from 'torchtext.data' (/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tokenizer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BucketIterator\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BucketIterator' from 'torchtext.data' (/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import BucketIterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(line):\n",
    "#     return line.split()\n",
    "\n",
    "def process_text_iter(text_iter):\n",
    "    texts, labels = [], []\n",
    "    for label, line in text_iter:\n",
    "        texts.append(line)\n",
    "        labels.append(label)\n",
    "    return texts, labels\n",
    "    #     yield {'text': tokenize(line), 'label': label}\n",
    "\n",
    "train_text, train_label = process_text_iter(train_iter)\n",
    "test_text, test_label = process_text_iter(test_iter)\n",
    "\n",
    "\n",
    "# train = []\n",
    "# for label, line in train_iter:\n",
    "#     train.append(\n",
    "#         {'text': tokenize(line), 'label': label}\n",
    "#     )\n",
    "# train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_tokenized_texts = [tokenizer(text) for text in train_text]\n",
    "test_tokenized_texts = [tokenizer(text) for text in test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for tokens in data_iter:\n",
    "        yield tokens\n",
    "    \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_tokenized_texts), max_tokens=25000, specials=[\"<unk>\", '<pad>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vectors = torch.zeros(len(vocab), glove.dim)\n",
    "for i, word in enumerate(vocab.get_itos()):  # get_itos() returns the list of words in vocab\n",
    "    if word in glove.stoi:  # stoi -> String-to-Index\n",
    "        embedding_vectors[i] = glove[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexed_sequences = [[vocab[token] for token in tokens] for tokens in train_tokenized_texts]\n",
    "test_indexed_sequences = [[vocab[token] for token in tokens] for tokens in test_tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([317, 254, 101,  ..., 654, 159, 494])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "max_length = max(len(seq) for seq in train_indexed_sequences)\n",
    "padded_sequences = rnn_utils.pad_sequence(\n",
    "    [torch.tensor(seq) for seq in train_indexed_sequences],\n",
    "    batch_first=True,\n",
    "    padding_value=vocab[\"<pad>\"]\n",
    ")\n",
    "padded_sequences\n",
    "\n",
    "sequence_lengths = torch.tensor([len(seq) for seq in train_indexed_sequences])\n",
    "sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1722])\n",
      "torch.Size([1000, 1722, 100])\n",
      "tensor([1000, 1000, 1000,  ...,    1,    1,    1])\n",
      "tensor([1000, 1000, 1000,  ...,    1,    1,    1])\n",
      "torch.Size([1, 1000, 256])\n",
      "torch.Size([1000, 1212, 256])\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        # vocab_size, \n",
    "        # embedding_dim, \n",
    "        embedding_vectors,\n",
    "        hidden_size,\n",
    "        pad_idx\n",
    "    ):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_vectors, \n",
    "            padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.embedding.weight.shape[1], \n",
    "            hidden_size, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size, \n",
    "            2\n",
    "        )\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        print(text.shape)\n",
    "        embedded = self.embedding(text)\n",
    "        print(embedded.shape)\n",
    "\n",
    "        packed_input = rnn_utils.pack_padded_sequence(\n",
    "            embedded, text_lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        print(packed_input.batch_sizes)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "\n",
    "        print(packed_output.batch_sizes)\n",
    "\n",
    "        print(hidden.shape)\n",
    "        output, _ = rnn_utils.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        print(output.shape)\n",
    "\n",
    "        return output\n",
    "pad_idx = '<pad>'\n",
    "model = RNN(\n",
    "    embedding_vectors=embedding_vectors,\n",
    "    hidden_size=256,\n",
    "    pad_idx=vocab[pad_idx]\n",
    ")\n",
    "\n",
    "model    \n",
    "data = model(padded_sequences[:1000], sequence_lengths[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1212, 256])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokenized_texts[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
