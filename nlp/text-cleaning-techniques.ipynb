{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text\n",
    "\n",
    "- Tokenization\n",
    "- Lematization\n",
    "- Stemming\n",
    "- Stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually\n",
    "\n",
    "I'm not being very critical here, just reviewing the general concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs to sentences:\n",
      "My name is Edy, I'm Data Scientist, and I like to play guitar. My girlfriend likes to watch TV Series.\n",
      "[\"My name is Edy, I'm Data Scientist, and I like to play guitar\", ' My girlfriend likes to watch TV Series']\n"
     ]
    }
   ],
   "source": [
    "# Paragraph -> Sentences\n",
    "paragraph = \"My name is Edy, I'm Data Scientist, and I like to play guitar. My girlfriend likes to watch TV Series.\"\n",
    "sentences = [t for t in paragraph.split('.') if t != '']\n",
    "\n",
    "print('Paragraphs to sentences:')\n",
    "print(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: My | name | is | Edy, | I'm | Data | Scientist, | and | I | like | to | play | guitar. | My | girlfriend | likes | to | watch | TV | Series.\n"
     ]
    }
   ],
   "source": [
    "# Paragraph to Words\n",
    "words = []\n",
    "\n",
    "splitted_words = paragraph.split(' ')\n",
    "for word in splitted_words:\n",
    "    words.append(word)\n",
    "\n",
    "words = [w for w in words if w != '']\n",
    "print('Words:', ' | '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK\n",
    "\n",
    "Note the differences between each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: My | name | is | Edy | , | I | 'm | Data | Scientist | , | and | I | like | to | play | guitar | . | My | girlfriend | likes | to | watch | TV | Series | .\n"
     ]
    }
   ],
   "source": [
    "# The quotation mark is attached to its next word, and the final stop is isolated.\n",
    "words = word_tokenize(paragraph)\n",
    "print('Words:', ' | '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: My | name | is | Edy | , | I | ' | m | Data | Scientist | , | and | I | like | to | play | guitar | . | My | girlfriend | likes | to | watch | TV | Series | .\n"
     ]
    }
   ],
   "source": [
    "# The quotation mark is separated from the other words\n",
    "words = wordpunct_tokenize(paragraph)\n",
    "print('Words:', ' | '.join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: My | name | is | Edy | , | I | 'm | Data | Scientist | , | and | I | like | to | play | guitar. | My | girlfriend | likes | to | watch | TV | Series | .\n"
     ]
    }
   ],
   "source": [
    "# The full stop is attached to its previous word\n",
    "words = TreebankWordTokenizer().tokenize(paragraph)\n",
    "print('Words:', ' | '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\n",
    "    'training', 'trained', 'trains', 'gain', 'gained', 'gaining',\n",
    "    'doing', 'monstruous', 'better', 'bettering', 'bettered', 'betterer'\n",
    "]\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemming:\n",
      "\n",
      "training        -> train\n",
      "trained         -> train\n",
      "trains          -> train\n",
      "gain            -> gain\n",
      "gained          -> gain\n",
      "gaining         -> gain\n",
      "doing           -> do\n",
      "monstruous      -> monstruou\n",
      "better          -> better\n",
      "bettering       -> better\n",
      "bettered        -> better\n",
      "betterer        -> better\n"
     ]
    }
   ],
   "source": [
    "print('PorterStemming:\\n')\n",
    "for word, stem in zip(words, [PorterStemmer().stem(w) for w in words]):\n",
    "    print(f'{word:<15} -> {stem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 5), (2, 3), (2, 1), (4, 4)]\n"
     ]
    }
   ],
   "source": [
    "def element(x):\n",
    "    return x[0]\n",
    "\n",
    "def sort_list(t):\n",
    "    return sorted(t, key=element)\n",
    "\n",
    "print(sort_list([(2, 5), (1, 2), (4, 4), (2, 3), (2, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemming:\n",
      "\n",
      "training        -> train\n",
      "trained         -> train\n",
      "trains          -> train\n",
      "gain            -> gain\n",
      "gained          -> gain\n",
      "gaining         -> gain\n",
      "doing           -> do\n",
      "monstruous      -> monstruous\n",
      "better          -> better\n",
      "bettering       -> better\n",
      "bettered        -> better\n",
      "betterer        -> better\n"
     ]
    }
   ],
   "source": [
    "# The only difference for the PorterStemmer was the word 'monstruous''\n",
    "print('PorterStemming:\\n')\n",
    "for word, stem in zip(words, [SnowballStemmer('english').stem(w) for w in words]):\n",
    "    print(f'{word:<15} -> {stem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming, but the resulting word always has the same meaning as the original one. The result of lemmatization is called \"lemma\", which is a root word.\n",
    "\n",
    " Stemming may reduce the word \"running\" to \"run\", but it could also reduce the word \"better\" to \"bet\", which doesn't keep the original meaning. On the other hand, lemmatization will reduce \"running\" to \"run\" and \"better\" to \"good\", which keeps the original meaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
